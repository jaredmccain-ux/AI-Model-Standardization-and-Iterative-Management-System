import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
import random
import numpy as np
from PIL import Image
import torchvision.transforms as T
import yaml
import os

from ultralytics.models.yolo.classify.train import ClassificationTrainer
from ultralytics.utils import loss

class ClsDistillationTrainer(ClassificationTrainer):
    """
    ä¸€ä¸ªç”¨äºå›¾åƒåˆ†ç±»çš„çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ã€‚
    - æ”¯æŒæ—§æ ·æœ¬å›æ”¾ (Replay)
    - æ”¯æŒä¸€è‡´æ€§æ­£åˆ™åŒ– (Consistency)
    - è’¸é¦å†…å®¹ä¸ºæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹è¾“å‡ºçš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ (logits)ã€‚
    """
    def __init__(self, *args, **kwargs):
        # --- 1. æ‹¦æˆªå¹¶ç§»é™¤è‡ªå®šä¹‰å‚æ•° ---
        overrides = kwargs.get('overrides', {})
        self.teacher_model = overrides.pop('teacher_model', None)
        self.distill_cls_weight = overrides.pop('distill_cls_weight', 1.0)
        self.temperature = overrides.pop('temperature', 2.0)
        
        # Replay params
        self.old_data_yaml = overrides.pop('old_data_yaml', None)
        self.replay_ratio = overrides.pop('replay_ratio', 0.3)
        self.replay_distill_boost = overrides.pop('replay_distill_boost', 2.0) # ç¡®ä¿æ‹¦æˆªæ­¤å‚æ•°
        self.max_replay_samples = overrides.pop('max_replay_samples', 500)
        
        # Consistency params
        self.enable_consistency = overrides.pop('enable_consistency', False)
        self.consistency_weight = overrides.pop('consistency_weight', 1.0)
        
        # ç§»é™¤å…¶ä»–ä»»åŠ¡å¯èƒ½ä¼ å…¥çš„æ— æ•ˆå‚æ•°ï¼Œé¿å…å†²çª
        overrides.pop('distill_reg_weight', None)
        overrides.pop('distill_feat_weight', None)
        overrides.pop('distill_mask_weight', None)
        overrides.pop('distill_bg_weight', None)
        overrides.pop('class_weights', None)
        overrides.pop('pseudo_conf_threshold', None)
        overrides.pop('new_class_ids', None) # ç¡®ä¿æ‹¦æˆªæ­¤å‚æ•°

        if self.teacher_model is None:
            raise ValueError("ClsDistillationTrainer requires a 'teacher_model' argument.")

        # --- 2. è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•° ---
        super().__init__(*args, **kwargs)

        # --- 3. åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹ ---
        self.teacher_model.to(self.device)
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False
        
        print("âœ… åˆ†ç±»ä»»åŠ¡è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸï¼Œæ•™å¸ˆæ¨¡å‹å·²å†»ç»“ã€‚")
        print(f"   - è’¸é¦æƒé‡ (Cls): {self.distill_cls_weight}, æ¸©åº¦: {self.temperature}")
        
        # --- 4. åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒº ---
        self.replay_buffer = []
        if self.old_data_yaml:
            self._load_replay_buffer()

        # --- 5. å®šä¹‰å¼ºå¢å¼º ---
        self.strong_aug = T.Compose([
            T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
            T.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),
        ])

        # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼ŒKLDivLossçš„reduction='batchmean'æ›´å¸¸ç”¨
        self.distill_cls_loss = nn.KLDivLoss(reduction='batchmean')

        # æ˜ å°„æ ‡è®°
        self.distill_indices_computed = False
        self.t_indices = None
        self.s_indices = None

    def _load_replay_buffer(self):
        print(f"\nğŸ“¦ æ­£åœ¨æ„å»ºæ—§æ ·æœ¬å›æ”¾ç¼“å†²åŒº (Classify)...")
        try:
            old_data_path = Path(self.old_data_yaml)
            train_dir = None
            
            # å°è¯•è§£æYAML
            if old_data_path.is_file() and old_data_path.suffix in ['.yaml', '.yml']:
                with old_data_path.open('r', encoding='utf-8') as f:
                    cfg = yaml.safe_load(f)
                    # Classification yaml usually has 'path' or 'train' pointing to dir
                    base = Path(cfg.get('path', old_data_path.parent))
                    if 'train' in cfg:
                        train_dir = base / cfg['train']
            elif old_data_path.is_dir():
                # å‡è®¾ç›´æ¥æä¾›äº†æ•°æ®é›†æ ¹ç›®å½•ï¼Œä¸”åŒ…å« 'train' å­ç›®å½•
                if (old_data_path / 'train').exists():
                    train_dir = old_data_path / 'train'
                else:
                    train_dir = old_data_path # æˆ–è€…æ˜¯ç›´æ¥çš„trainç›®å½•
            
            if not train_dir or not train_dir.exists():
                print(f"   âŒ æ— æ³•å®šä½æ—§æ•°æ®çš„è®­ç»ƒç›®å½•: {train_dir}")
                return

            print(f"   - æ‰«æç›®å½•: {train_dir}")
            
            # è·å–æ•™å¸ˆæ¨¡å‹çš„ç±»åˆ«åç§°
            teacher_names = self.teacher_model.names # dict {0: 'name', ...}
            valid_class_names = set(teacher_names.values())
            
            valid_samples = []
            # éå†ç›®å½•
            for class_dir in train_dir.iterdir():
                if class_dir.is_dir() and class_dir.name in valid_class_names:
                    for img_file in class_dir.glob('*.*'):
                        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.webp']:
                            valid_samples.append({
                                'img_path': str(img_file),
                                'class_name': class_dir.name
                            })
            
            if len(valid_samples) > self.max_replay_samples:
                self.replay_buffer = random.sample(valid_samples, self.max_replay_samples)
            else:
                self.replay_buffer = valid_samples
                
            print(f"   âœ… æˆåŠŸåŠ è½½ {len(self.replay_buffer)} ä¸ªæ—§æ ·æœ¬")

        except Exception as e:
            print(f"   âŒ åŠ è½½æ—§æ ·æœ¬å¤±è´¥: {e}")

    def preprocess_batch(self, batch):
        batch = super().preprocess_batch(batch)
        
        # Replay
        if self.replay_buffer and self.replay_ratio > 0:
            batch_size = batch['img'].shape[0]
            num_replay = int(batch_size * self.replay_ratio)
            if num_replay > 0:
                indices = random.sample(range(batch_size), num_replay)
                for idx in indices:
                    sample = random.choice(self.replay_buffer)
                    # Load Image
                    try:
                        img = Image.open(sample['img_path']).convert('RGB')
                        img = img.resize((self.args.imgsz, self.args.imgsz))
                        # Convert to tensor
                        img_t = T.ToTensor()(img).to(self.device)
                        batch['img'][idx] = img_t
                        
                        # Update Label
                        # Find class index in current model
                        # self.model.names is dict {id: name}
                        # We need name -> id
                        name_to_id = {v: k for k, v in self.model.names.items()}
                        if sample['class_name'] in name_to_id:
                            batch['cls'][idx] = name_to_id[sample['class_name']]
                    except Exception:
                        pass

        # Consistency
        if self.enable_consistency:
            batch['img_weak'] = batch['img'].clone()
            try:
                batch['img'] = self.strong_aug(batch['img'])
            except:
                pass
                
        return batch

    def get_loss(self, preds, batch):
        # --- 1. è®¡ç®—æ ‡å‡†GTæŸå¤± ---
        # ClassificationTrainerç›´æ¥å°†æŸå¤±å‡½æ•°å®ä¾‹ä¿å­˜åœ¨self.criterion
        loss_gt, loss_items = self.criterion(preds, batch)

        # --- 2. å‡†å¤‡å­¦ç”Ÿå’Œæ•™å¸ˆçš„è¾“å‡º ---
        student_preds = preds
        
        # Teacher input
        teacher_img = batch.get('img_weak', batch['img']) if self.enable_consistency else batch['img']
        
        with torch.no_grad():
            teacher_preds = self.teacher_model(teacher_img)

        # --- 3. è®¡ç®—åˆ†ç±»è’¸é¦æŸå¤± (å¸¦ç±»åˆ«æ˜ å°„) ---
        # åŠ¨æ€è®¡ç®—ç±»åˆ«æ˜ å°„ï¼Œç¡®ä¿å³ä½¿ç±»åˆ«é¡ºåºä¸åŒæˆ–æœ‰æ–°å¢ç±»åˆ«ä¹Ÿèƒ½æ­£ç¡®è’¸é¦
        if not self.distill_indices_computed:
            self.t_indices = []
            self.s_indices = []
            
            t_names = self.teacher_model.names
            s_names = self.model.names
            
            # åè½¬å­¦ç”Ÿæ¨¡å‹åç§°å­—å…¸ä»¥ä¾¿æŸ¥æ‰¾
            s_name_to_id = {v: k for k, v in s_names.items()}
            
            # æŸ¥æ‰¾å…¬å…±ç±»åˆ«
            for t_id, t_name in t_names.items():
                if t_name in s_name_to_id:
                    self.t_indices.append(t_id)
                    self.s_indices.append(s_name_to_id[t_name])
            
            # è½¬ä¸ºTensor
            if self.t_indices:
                self.t_indices = torch.tensor(self.t_indices, device=self.device, dtype=torch.long)
                self.s_indices = torch.tensor(self.s_indices, device=self.device, dtype=torch.long)
            
            self.distill_indices_computed = True
            
            print(f"â„¹ï¸  è’¸é¦ç±»åˆ«æ˜ å°„å·²å»ºç«‹: å…±æœ‰ {len(self.t_indices)}/{len(t_names)} ä¸ªæ•™å¸ˆç±»åˆ«è¢«åŒ¹é…ã€‚")
            if len(self.t_indices) < len(t_names):
                print("âš ï¸  è­¦å‘Š: å­¦ç”Ÿæ¨¡å‹ç¼ºå°‘éƒ¨åˆ†æ•™å¸ˆæ¨¡å‹çš„ç±»åˆ«ï¼Œè¿™äº›ç±»åˆ«çš„çŸ¥è¯†å°†æ— æ³•è¢«è’¸é¦ã€‚")

        # ä»…åœ¨æœ‰å…¬å…±ç±»åˆ«æ—¶è®¡ç®—è’¸é¦æŸå¤±
        if self.t_indices is not None and len(self.t_indices) > 0:
            # æå–å¯¹åº”çš„logitså­é›†
            s_logits_subset = student_preds[:, self.s_indices]
            t_logits_subset = teacher_preds[:, self.t_indices]
            
            # ä½¿ç”¨ log_softmax å’Œ softmax æ¥è®¡ç®—KLæ•£åº¦
            loss_distill_cls = self.distill_cls_loss(
                F.log_softmax(s_logits_subset / self.temperature, dim=1),
                F.softmax(t_logits_subset / self.temperature, dim=1)
            ) * (self.temperature ** 2) # T^2 scaling
        else:
            loss_distill_cls = torch.tensor(0.0, device=self.device)

        # Consistency weight
        c_weight = self.consistency_weight if self.enable_consistency else 1.0

        # --- 4. åˆå¹¶æ‰€æœ‰æŸå¤± ---
        total_loss = loss_gt + self.distill_cls_weight * loss_distill_cls * c_weight

        new_loss_items = torch.tensor([loss_distill_cls], device=self.device)
        # åˆ†ç±»ä»»åŠ¡çš„loss_itemsé€šå¸¸åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œç›´æ¥æ‹¼æ¥
        loss_items = torch.cat((loss_items, new_loss_items))
        
        return total_loss, loss_items